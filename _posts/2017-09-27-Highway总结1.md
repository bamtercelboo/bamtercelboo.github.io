---
layout:     post
title:      "Highway Networks总结"
date:       2017-09-27
author:     "bamtercelboo"
header-img: "img/post-bg-2015.jpg"
tags:
    - 博客园
---

## （一）Highway Networks 与 Deep Networks 的关系##

>  深层神经网络相比于浅层神经网络具有更好的效果，在很多方面都已经取得了很好的效果，特别是在图像处理方面已经取得了很大的突破，然而，伴随着深度的增加，深层神经网络存在的问题也就越大，像大家所熟知的梯度消失问题，这也就造成了训练深层神经网络困难的难题。2015年由Rupesh Kumar Srivastava等人受到LSTM门机制的启发提出的网络结构（Highway Networks）很好的解决了训练深层神经网络的难题，Highway Networks 允许信息**高速无阻碍**的通过深层神经网络的各层，这样有效的减缓了梯度的问题，使深层神经网络不在仅仅具有浅层神经网络的效果。

## （二）Deep Networks 梯度消失/爆炸（vanishing and exploding gradient）问题##

> 我们先来看一下简单的深层神经网络（仅仅几个隐藏层）
> 
> ![](https://i.imgur.com/eg3bMXJ.png)
> 
> 先把各个层的公式写出来

> C=sigmoid(W_4*H_3  +b_4)
> 
> H_3=sigmoid(W_3*H_2  +b_3)
> 
> H_2=sigmoid(W_2*H_1  +b_2)
> 
> H_1=sigmoid(W_1*x +b_1)
> 
> 对W_1求导
> 
> ![](https://i.imgur.com/yYlxgbB.gif)
> 
> W=W - lr * g(t)
> 
> 以上公式仅仅是四个隐藏层的情况，当隐藏层的数量达到数十层甚至是数百层的情况下，一层一层的反向传播回去，当权值 < 1的时候，反向传播到某一层之后权值近乎不变，相当于输入x的映射，例如，g(t) =〖0.9〗^100已经是很小很小了，这就造成了只有前面几层能够正常的反向传播，后面的那些隐藏层仅仅相当于输入x的权重的映射，权重不进行更新。反过来，当权值 > 1的时候，会造成梯度爆炸，同样是仅仅前面的几层能更改正常学习，后面的隐藏层会变得很大很大。


## （三）Highway Networks Formula##

- **Notation**
> （.）操作代表的是矩阵按位相乘
> 
> sigmoid函数：sigmoid=  1/(1+e^(-x) )

- **Highway Networks formula**
> 对于我们普通的神经网络，用非线性激活函数H将输入的x转换成y,公式1忽略了bias。但是，H不仅仅局限于激活函数，也采用其他的形式，像convolutional和recuerrent。

> ![](https://i.imgur.com/mZLjQcH.png)
> 
> 对于Highway Networks神经网络，增加了两个非线性转换层，一个是 T（transform gate） 和一个是 C（carry gate），通俗来讲，T表示输入信息经过convolutional或者是recuerrent的信息被转换的部分，C表示的是原始输入信息x保留的部分 ，其中 T=sigmoid(wx + b) 
> 
> ![](https://i.imgur.com/GHOGFtH.png)
> 
> 为了计算方便，这里定义了 C =  1 - T
> 
> ![](https://i.imgur.com/BGkRdf3.png)
> 
> 需要注意的是x，y， H， T的维度必须一致，要想保证其维度一致，可以采用sub-sampling或者zero-padding策略，也可以使用普通的线性层改变维度，使其一致，可以采用几个公式相比，公式3要比公式1灵活的多，可以考虑一下特殊的情况，T= 0的时候，y = x，原始输入信息全部保留，不做任何的改变，T = 1的时候，Y = H，原始信息全部转换，不在保留原始信息，仅仅相当于一个普通的神经网络。
> 
> ![](https://i.imgur.com/xXwlRys.png)
> 
> ![](https://i.imgur.com/C6sjdpM.png)
> 

## （四）Highway Networks 实验结果 ##

- **个人实验结果**
> 
> ![](https://i.imgur.com/G9Czg8F.jpg)
> 
> 以上实验结果是在情感分类数据集中测试的结果。

>从图中可以看出，相同的参数情况下，浅层神经网络相互对比变化不是很明显，5层的神经网络就有了一些变化，准确率相差了一个点左右。由于硬件资源，更加深的深层神经网络还没有测试。

> 但是从图中也可以发现问题就是伴随深度的加深，Highway Networks的准确率也在下降，深度加深，神经网络的参数也就增加的越多，这就需要重新调节超参数。

- **Paper 实验结果**
> ![](https://i.imgur.com/zOukeYJ.jpg)
> 
> 从论文的实验结果来看，当深层神经网络的层数能够达到50层甚至100层的时候，loss也能够下降的很快，犹如几层的神经网络一样，与普通的深层神经网络形成了鲜明的对比。


## （五）Demo ##
> 在pytorch上实现了多个Highway Networks，其中包括单纯的Highway Networks，以及convolution Highway Networks、LSTm Highway Networks以及Highway Networks的一些变种。
> 
>github链接： [Highway Networks implement in pytorch](https://github.com/bamtercelboo/pytorch_Highway_Networks) 

## References ##
- [Highway Networks(paper)](https://arxiv.org/pdf/1505.00387.pdf)

- [Training Very Deep Networks](https://arxiv.org/pdf/1507.06228.pdf)

- [为什么深层神经网络难以训练](http://blog.csdn.net/binchasing/article/details/50300069)

- [Training Very Deep Networks--Highway Networks ](http://blog.csdn.net/cv_family_z/article/details/50349436)

- [Very Deep Learning with Highway Networks](http://people.idsia.ch/~rupesh/very_deep_learning/)

- [Hightway Networks学习笔记 ](http://blog.csdn.net/sinat_35218236/article/details/73826203?utm_source=itdadao&utm_medium=referral)