---
layout:     post
title:      "cw2vec理论及其实现"
date:       2018-05-11
author:     "bamtercelboo"
header-img: "img/post-bg-2015.jpg"
tags:
    - cw2vec
    - word2vec
    - Word Embedding
    - c++

---


#  导读  #

本文对AAAI 2018(Association for the Advancement of Artificial Intelligence 2018)高分录用的一篇中文词向量论文（[cw2vec: Learning Chinese Word Embeddings
with Stroke n-gram Information](http://www.statnlp.org/wp-content/uploads/papers/2018/cw2vec/cw2vec.pdf)）进行简述与实现，这篇论文出自蚂蚁金服人工智能部。本文将从背景知识、模型简介、c++实现、实验结果、结论等几个方面来进行阐述。

# 一、背景知识 #

目前已经存在很多的词向量模型，但是较多的词向量模型都是基于西方语言，像英语，西班牙语，德语等，这些西方语言的内部组成都是拉丁字母，然而，由于中文书写和西方语言完全不同，中文词语包含很少的中文字符，但是中文字符内部包含了很强的语义信息，因此，如何有效利用中文字符内部的语义信息来训练词向量，成为近些年研究的热点。  

通过观察中文字符内部组成，发现中文字符包含偏旁部首、字符组件，笔画信息等语义信息特征（如下图），基于偏旁部首和汉字组件特征的中文词向量模型已经有人提出，并取得了较好的效果。  
![](https://i.imgur.com/qrCb8to.jpg)  

本篇论文采用笔画信息作为特征，由于每个字符包含很多的笔画，类似于一个英文单词包含很多的拉丁字母，在这个基础之上，提出了笔画的n-gram特征。这个思想来源于2016年facebook提出的论文（[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)），目前facebook这篇论文已经被引用300多次，影响力很大，cw2vec可以称之为中文版本的fasttext。

# 二、模型简介 #

#### 1、 词语分割 ####

把中文词语分割为单个字符，为了获取中文字符的笔画信息。
> 词语：大人  分割为：（1）大 （2）人    

#### 2、 笔画特征 ####

获取中文字符的笔画信息，并且把字符的笔画信息合并，得到词语的笔画信息。
> 大： 一ノ丶   
> 人： ノ丶  
> 大人： 一ノ丶 ノ丶  


#### 3、 笔画特征数字化 ####

为了方便，论文提及把笔画信息数字化，用数字代表每一种笔画信息，如下图。  
![](https://i.imgur.com/gx44sVQ.jpg)  
那么“大人”这个词的笔画信息就可以表示为：
> 大人： 一ノ丶 ノ丶  
> 大人：13434
  
我从训练语料中获取到13354个汉字，并获取笔画信息，统计笔画种类和上图一致，只有5种笔画信息。  

#### 4、 N元笔画特征 ####
提取词语笔画信息的n-gram特征。
> 3-gram：134、343、434  
> 4-gram：1343、3434  
> 5-gram：13434  
> ......

上述4个步骤，如下图：  
![](https://i.imgur.com/oYE08t5.jpg)

#### 5、cw2vec模型  ####
word2vec提出了CBOW和Skip-Gram两个模型（[详解](https://bamtercelboo.github.io/2018/03/24/word2vec/)），cw2vec在Skip-Gram基础之上进行改进，把**词语的n-gram笔画特征信息代替词语**进行训练，cw2vec模型如下图。    
> 短语：治理 雾霾 刻不容缓  
> 中心词：雾霾  
> 上下文词：治理，刻不容缓

![](https://i.imgur.com/OROhByA.jpg)

论文中提及上下文词向量（`context word embedding`）为最终cw2vec模型的输出词向量。


# 三、c++实现 #

论文目前还没有公开代码，在这里，我提供一个c++版本的cw2vec（[cw2vec-github](https://github.com/bamtercelboo/cw2vec)）。这份代码不仅仅实现了`cw2vec`模型，还包括word2vec的 `CBOW` 和 `Skip-Gram` 两个模型，以及FaceBook提出的`fasttext`。关于这版的CBOW和Skip-Gram的性能，可以看一下[word2vec](https://bamtercelboo.github.io/2018/03/24/word2vec/)，有简单的性能测试，以下是四个模型。

	./word2vec    
	skipgram  ------ train word embedding by use skipgram model  
	cbow      ------ train word embedding by use cbow model  
	subword   ------ train word embedding by use subword(fasttext skipgram)  model  
	substoke  ------ train chinses character embedding by use substoke(cw2vec) model    

可以根据 `-h` 参数设置   

	./word2vec skipgram -h  
	The Following arguments are mandatory:
		-input                   training file path
		-infeature               substoke feature file path
		-output                  output file path
	......  

subword是fasttext skipgram模型，substoke是cw2vec模型。substoke 需要笔画信息的特征文件，这里已经写好了一份脚本从[汉典](http://www.zdic.net/)抽取笔画信息（[extract_zh_char_stoke](https://github.com/bamtercelboo/corpus_process_script/tree/master/extract_zh_char_stoke)），具体使用查看README，`特征文件`类似于：

	中 丨フ一丨
	国 丨フ一一丨一丶一
	庆 丶一ノ一ノ丶
	假 ノ丨フ一丨一一フ一フ丶
	期 一丨丨一一一ノ丶ノフ一一
	......

cw2vec论文提出采用`context word embedding` 作为最终的词向量，在这份代码中，不仅仅考虑了context word embedding ，而且，根据fasttext提供的思路，把笔画信息的n-gram特征取平均作为最后的词向量，所以，substoke模型会输出两份词向量，vec代表`context word embedding`，avg代表笔画信息的`n-gram特征取平均`。  

需要包括的环境：
  
>cmake  
>make  
>gcc  

更多信息查看[cw2vec-github](https://github.com/bamtercelboo/cw2vec)。


# 四、实验结果 #

#### 1、训练数据  ####

训练数据采用最新的中文维基百科训练语料（[处理过程](https://bamtercelboo.github.io/2018/05/10/wikidata_Process/)），利用[jieba分词工具](https://github.com/fxsjy/jieba)对语料进行分词处理，最终得到1.2G的训练数据，根据论文，我们从1.2G的训练语料中获取前20%数据，大约260M左右，我们用这份数据在`Word Similarity`任务上进行了评测。

#### 2、参数设置  ####

在对比实验中，几个模型的参数设置如下所示：  

	设置参数如下：
		词向量维度：100
		窗口大小：5
		负采样数目：5
		迭代次数：5
		最小词频：10
		学习率：skipgram(0.025)，cbow(0.05)，substoke(0.025)
		n-gram特征：minn=3, maxn=18

#### 3、实验结果  ####

在中文`word similarity`任务上进行了评测，评测文件是 `wordsim-240` 和 `wordsim-296`，这两份文件来自于`Chen et al. 2015; Xu et al. 2016`，基于上述两份文件，我已经写好了一份中文词相似度评测脚本（[Chinese-Word-Similarity-and-Word-Analogy](https://github.com/bamtercelboo/Chinese_Word_Similarity_and_Word_Analogy)），方便大家使用，详细信息看README，下面是实验结果。  

下图和表中的`substoke-average`代表使用的词向量是笔画信息的`n-gram特征取平均`，`substoke-context`代表使用的词向量是`context word embedding`。  

![](https://i.imgur.com/Q41552Q.jpg)    

![](https://i.imgur.com/cb5nmv0.jpg)


#### 4、结果分析  ####
从上面的实验结果来看，在`word similarity`任务上，总体来看，`substoke`模型要比`skipgram`，`cbow`两个模型结果要好。其中在`wordsim-240`上，substoke的两份词向量都要比word2vec的好，在`wordsim-296`上，`substoke-context`表现的相对差一些，但是`substoke-average`要好很多。  
由于时间原因，目前仅在相似度任务上进行了评测，后续可能会在词汇类比，词性标注，命名实体识别，文本分类等多个上进行评测。

# 五、结论 #
中文字符内部结构包含了丰富的语义信息，这篇论文提出的思路，挖掘了中文字符笔画特征信息，上述实验证明是有效的，推动了中文词向量的工作进展，后续研究者可以在此基础之上进行实验，更加深层次的挖掘中文字符内部信息。


# References  #
[1] Cao, Shaosheng, et al. "cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information." (2018).  
[2] Bojanowski, Piotr, et al. "Enriching word vectors with subword information." arXiv preprint arXiv:1607.04606 (2016).  
[3] Chen, Xinxiong, et al. "Joint Learning of Character and Word Embeddings." IJCAI 2015.  
[4] Sun, Yaming, et al. "Radical-enhanced Chinese character embedding." ICNIP 2014.  
[5] Li, Yanran, et al. "Component-enhanced Chinese character embeddings." arXiv preprint arXiv:1508.06669 (2015).  
[6] Yu, Jinxing, et al. "Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components." EMNLP 2017.  
[7] Mikolov, Tomas, et al. "Efficient estimation of word representations in vector space." arXiv preprint arXiv:1301.3781 (2013).





