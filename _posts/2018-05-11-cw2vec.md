---
layout:     post
title:      "cw2vec理论及其实现"
date:       2018-05-11
author:     "bamtercelboo"
header-img: "img/post-bg-2015.jpg"
tags:
    - cw2vec
    - word2vec
    - Word Embedding
    - c++

---


#  导读  #
本文对AAAI 2018(Association for the Advancement of Artificial Intelligence 2018)高分录用的一篇中文词向量论文（[cw2vec: Learning Chinese Word Embeddings
with Stroke n-gram Information](http://www.statnlp.org/wp-content/uploads/papers/2018/cw2vec/cw2vec.pdf)）进行简述与实现，这篇论文出自蚂蚁金服人工智能部。本文将从背景知识、模型简介、c++实现、实验结果、结论等几个方面来进行阐述。

# 背景知识 #
目前已经存在很多的词向量模型，但是较多的词向量模型都是基于西方语言，像英语，西班牙语，德语等，这些西方语言的内部组成都是拉丁字母，然而，由于中文书写和西方语言完全不同，中文词语包含很少的中文字符，而且中文字符内部包含了很强的语义信息，如何利用中文字符内部的语义信息来训练词向量，针对中文字符的特点，近些年来，有越来越多的研究者提出了各种各样的中文词向量模型。  

通过观察中文字符内部组成，发现中文字符偏旁部首、汉字组件，笔画信息等特征（如下图），基于偏旁部首和汉字组件特征的中文词向量模型已经有人提出，并取得了较好的效果。  
![](https://i.imgur.com/qrCb8to.jpg)  

本篇论文采用笔画信息作为特征，由于每个字符包含很多的笔画，类似于一个英文单词包含很多的拉丁字母，在这个基础之上，提出了笔画的n-gram特征。这个思想来源于2016年facebook提出的论文（[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)），目前facebook这篇论文已经被引用300多次，影响力很大，cw2vec可以称之为中文本的fasttext。

# 模型简介 #

# c++实现 #

# 实验结果 #

# 结论 #

# References  #
[1] Cao, Shaosheng, et al. "cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information." (2018).  
[2] Bojanowski, Piotr, et al. "Enriching word vectors with subword information." arXiv preprint arXiv:1607.04606 (2016).  
[3] 





