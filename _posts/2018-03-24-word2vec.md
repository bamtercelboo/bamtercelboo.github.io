---
layout:     post
title:      "word2vec"
date:       2018-03-24
author:     "bamtercelboo"
header-img: "img/post-bg-2015.jpg"
tags:
    - word2vec, skip-gram, cbow, Hierarchical Softmax, Negative Sampling
---


# 导读  #

- 本文简单的介绍了Google 于 2013 年开源推出的一个用于获取 word vector 的工具包（word2vec），并且简单的介绍了其中的连个训练模型（Skip-gram，CBOW），以及两种加速的方法（Hierarchical Softmax，Negative Sampling）。

# 一 、word2vec  #

- word2vec最初是由Tomas Mikolov 2013年在ICLR发表的一篇文章[[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf "Efficient Estimation of Word Representations in Vector Space")]， 并且开源了代码，作用是将所有词语投影到K维的向量空间，每个词语都可以用一个K维向量表示。由于它简洁，高效的特点，引起了人们的广泛关注，并应用在很多NLP任务中，用于训练相应的词向量。

## 1、传统的词表示  --- one-hot representation ##

- 这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。

- 假如词表是：[气温、已经、开始、回升、了]，那么词的词向量分别可以是[1,0,0,0,0]，[0,1,0,0,0]，[0,0,1,0,0]，[0,0,0,1,0]，[0,0,0,0,1]。这样的表示方法简单容易理解，而且编程也很容易实现，只需要取对应的索引就能够完成，已经可以解决相当一部分NLP的问题，但是仍然存在不足，即词向量与词向量之间都是相互独立的；我们都知道，词与词之间是有一定的联系的，我们无法通过这种词向量得知两个词在语义上是否相似，并且如果词表非常大的情况下，每个词都是茫茫 0 海中的一个 1，这种高维稀疏的表示也有可能引发维度灾难。为了解决上述问题，就有了词向量的第二种表示方法。


## 2、Distributed representation --- word embedding ##

- word2vec就是通过这种方法将词表示为向量，即通过训练将词表示为限定维度K的实数向量，这种非稀疏表示的向量很容易求它们之间的距离(欧式、余弦等)，从而判断词与词语义上的相似性，也就解决了上述one-hot方法表示两个词之间的相互独立的问题。

- 不过Distributed representation并不是word2vec诞生才有的， Distributed representation 最早是 Hinton 在 1986 年的论文《[Learning distributed representations of concepts](http://www.cs.toronto.edu/~hinton/absps/families.pdf)》中提出的。虽然这篇文章没有说要将词做 Distributed representation，但至少这种先进的思想在那个时候就在人们的心中埋下了火种，到 2000 年之后开始逐渐被人重视。 word2vec之所以会产生这么大的影响，是因为它采用了简化的模型，使得训练速度大为提升，让word embedding这项技术(也就是词的distributed representation)变得实用，能够应用在很多的任务上。

# 二 、Skip-Gram model and CBOW  model#
- 我们先首先来看一下两个model的结构图。  

 ![](https://i.imgur.com/d6OFeFh.jpg)

- 上图示`CBOW`和`Skip-Gram`的结构图，从图中能够看出，两个模型都包含三层结构，分别是`输入层`，`投影层`，`输出层`；CBOW模型是在已知当前词上下文context的前提下预测当前词w(t)，类似阅读理解中的完形填空；而Skip-Gram模型恰恰相反，是在已知当前词w(t)的前提下，预测上下文context。

- 对于`CBOW`和`Skip-Gram`两个模型，word2vec给出了两套框架，用于训练快而好的词向量，他们分别是`Hierarchical Softmax` 和 `Negative Sampling`，下文将介绍这两种加速方法。

# 三 、Negative Sampling #
 
- `Negative Sampling（NEG）` 是Tomas Mikolov在[Distributed Representations of Words and Phrasesand their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)中提出的，它是`NCE`(Noise Contrastive Estimation)的简化版本，用于提高训练速度和提升词向量质量。

## 1、 Negative Sampling   ##

- 比如我们的训练样本，中心词是`w`，它周围上下文共有2c个词，记为`context(w)`。由于这个中心词`w`，的确和`context(w)`相关存在，因此它是一个真实的`正例`。通过`Negative Sampling`进行负采样，我们得到neg（负采样的个数）个和w不同的中心词`wi`，i=1,2,..neg，这样context(w)和wi就组成了neg个并不真实存在的`负例`。利用这一个正例和neg个负例，我们进行二元逻辑回归（可以理解成一个`二分类问题`），得到负采样对应每个词wi对应的模型参数以及每个词的词向量。

## 2、 How to do Negative Sampling?   ##
- 我们来看一下如何进行负采样，得到neg个负例。`word2vec`采样的方法并不复杂，如果词汇表的大小为V，那么我们就将一段长度为1的线段分成V份，每份对应词汇表中的一个词。当然每个词对应的线段长度是不一样的，高频词对应的线段长，低频词对应的线段短（根据词频采样，出现的次数越多，负采样的概率越大）。每个词w的线段长度由下式决定：
- ![](https://i.imgur.com/n398SNR.jpg)

- 在采样前，我们将这段长度为1的线段划分成M等份，这里M>>V，这样能够保证每个词对应的线段都会划分成对应的小块，而M份中每一份都会落在某一个词对应的线段上(如下图)，采样的时候，我们只需要随机生成neg个数，对应的位置就是采样的负例词。
- ![](https://i.imgur.com/kCanaQr.png)


# 四 、Hierarchical Softmax #

- update later


# 五 、Demo #

- 在这里提供了几份代码，包括我实现的`c++`，`pytorch`版本，以及word2vec源码版本及其源码注释版。

-  `pytorch version: https://github.com/bamtercelboo/pytorch_word2vec`

-  `cpp version: https://github.com/bamtercelboo/word2vec/tree/master/word2vec` 

-  `word2vec source version：word2vec.googlecode.com/svn/trunk/`

-  `word2vec annotation version: https://github.com/tankle/word2vec`



# 六 、Experiment Result #

- 我们在英文语料`enwiki-20150112_text.txt`（12G）上进行了测试，测试采用的是根据这篇论文 [Community Evaluation and Exchange of Word Vectors at wordvectors.org](http://www.aclweb.org/anthology/P14-5004) 提供的方法与site（[http://www.wordvectors.org/index.php](http://www.wordvectors.org/index.php)），计算词之间的相似度。
- 结果如下图所示：由于 `pytorch version` 训练速度慢并且demo还没有完善，所以仅在 `cpp version ` 和 ` word2vec源码` 进行了测试对比。以上对比试验均是在相同的参数设置下完成的。
- 参数设置
	- `model: skip-gram`
	- `loss: Negative Sampling`
	- `neg: 10`
	- `dim: 100`
	- `lr: 0.025`
	- `windows size: 5`
	- `minCount: 10`
	- `iters: 5`

- ![](https://i.imgur.com/bG5Wn7W.jpg)




## References  ##
[1] Tao Lei and Yu Zhang. Training RNNs as Fast as CNNs. arXiv:1709.02755, 2017.  
[2] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural
networks. In ICLR, 2017.  
[3] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information Processing Systems 29 (NIPS), 2016.  
[4] Jeremy Appleyard, Tomas Kocisky, and Phil Blunsom. Optimizing performance of recurrent neural networks on gpus. arXiv preprint arXiv:1604.01946, 2016.  


